{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from\n",
    "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb#scrollTo=V8oadoJSWp7C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1\n",
    "!pip install imageio\n",
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Virtual display\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "# virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
      "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-cr05a2dm\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-cr05a2dm\n",
      "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
      "  Cloning https://github.com/simoninithomas/gym-games to /tmp/pip-req-build-ok3_fe0k\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /tmp/pip-req-build-ok3_fe0k\n",
      "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.22.2)\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.4.9)\n",
      "Requirement already satisfied: pyyaml==6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (10.3.0)\n",
      "Requirement already satisfied: gym>=0.13.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.26.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (68.2.2)\n",
      "Requirement already satisfied: pygame>=1.9.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.5.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (24.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "# from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_id = \"ALE/Pong-v5\"\n",
    "# Create the env\n",
    "env = gym.make(env_id,obs_type=\"grayscale\",full_action_space=False)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id,obs_type=\"grayscale\",full_action_space=False)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = gym.spaces.utils.flatten_space(env.observation_space).shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  33600\n",
      "Sample observation [[ 39 197 216 ...  61   6 118]\n",
      " [210  70 181 ... 149 138  11]\n",
      " [ 52 235 129 ... 123 139 169]\n",
      " ...\n",
      " [132 218  46 ...  64  62  47]\n",
      " [160  74  82 ...  53 153 247]\n",
      " [121 136  20 ...  23  22 156]]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  6\n",
      "Action Space Sample 2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "save_dir = \"diffusion training data/\"\n",
    "\n",
    "# Store the state space and action space for the diffusion model to use later.\n",
    "observations = []\n",
    "actions = [] \n",
    "rewards = [] \n",
    "dones = [] # Store True if the frame ended the episode (terminate or truncate), False otherwise \n",
    "\n",
    "def add_transition(observation, action, reward, done):\n",
    "    \"\"\"\n",
    "    Observations: What the agent sees (dimensions are 33600 x 1)\n",
    "    Actions: What the agent does (dimensions are 6 x 1)\n",
    "    Rewards: The reward the agent gets (range is -1 to 1)\n",
    "    Dones: If the episode is over (True or False)\n",
    "    \"\"\"\n",
    "    observations.append(observation)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    dones.append(done)\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "def convert_to_tensors(observations,actions,rewards,dones):\n",
    "    #create lists to organize the training data.\n",
    "    inputs = []\n",
    "    truths = []\n",
    "\n",
    "    h = observations[0].shape[0] #height of a pong image\n",
    "    w = observations[0].shape[1] #width of a pong image\n",
    "\n",
    "    #take 2 images as input, return one image (the model should need two input images to determine ball velocity)\n",
    "    for i in range(len(observations)-2):\n",
    "        #expand action to a matrix the size of a pong image, and add it as a seperate channel in a tensor image later\n",
    "        act = np.broadcast_to(actions[i+1], (h,w) )\n",
    "        rew = rewards[i+2]\n",
    "        don = dones[i+2]\n",
    "\n",
    "\n",
    "        input = np.stack((observations[i], observations[i+1], act), axis=0) #put the channels together so my life is easier and the nn pays a lot of attention to act\n",
    "        truth = (observations[i+2],rew,don) #tuple of outputs\n",
    "        inputs.append(input)\n",
    "        truths.append(truth)\n",
    "\n",
    "\n",
    "    inputs = np.stack(inputs,axis=0) # shape of (n,3,h,w)\n",
    "    truths = (\n",
    "        np.expand_dims(np.stack([t[0] for t in truths],axis = 0), axis=1),#(n,1,h,w)\n",
    "        np.fromiter((t[1] for t in truths),dtype=\"float\"),#(n)\n",
    "        np.fromiter((t[2] for t in truths),dtype=\"float\")#(n)\n",
    "    )\n",
    "\n",
    "    # print(inputs.shape) # (n,3,h,w)\n",
    "    # print(truths[0].shape) # (n,1,h,w)\n",
    "    # print(truths[1].shape) # (n,)\n",
    "    # print(truths[2].shape) # (n,)\n",
    "\n",
    "    inputs = torch.tensor(inputs,device=device,dtype=torch.float)\n",
    "    truths = (\n",
    "        torch.tensor(truths[0],device=device,dtype=torch.float),\n",
    "        torch.tensor(truths[1],device=device,dtype=torch.float),\n",
    "        torch.tensor(truths[2],device=device,dtype=torch.float)\n",
    "    )\n",
    "    return inputs,truths\n",
    "\n",
    "def save_transitions():\n",
    "    \"\"\"\n",
    "    divide each array into sets of 1000 and save it in chunks, so it can be loaded with a dataloader.\n",
    "    \"\"\"\n",
    "    assert len(observations) == len(actions) == len(rewards) == len(dones)\n",
    "    #change bool in dones to floats\n",
    "    for i in range(len(dones)):\n",
    "        dones[i] = float(dones[i])\n",
    "\n",
    "    existing_files = [f for f in listdir(save_dir) if isfile(join(save_dir, f))]\n",
    "    j = None\n",
    "    if existing_files:\n",
    "        #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
    "        j = max([int(''.join([c for c in f if c.isdigit()])) for f in existing_files])\n",
    "    else:\n",
    "        j = -1\n",
    "    j += 1\n",
    "\n",
    "    n = 1000 #hardcoded.\n",
    "\n",
    "    #iterate while chunking it. \n",
    "    for i,obs,act,rew,don in zip(range(j,len(observations)),chunks(observations,n),chunks(actions,n),chunks(rewards,n),chunks(dones,n)):\n",
    "        if len(obs) < n: # discard if its not big enough for a chunk.\n",
    "            break\n",
    "        np.savez_compressed(file=save_dir + \"transitions{i}.npz\".format(i=i), observations = obs,actions=act,rewards=rew,dones=don)\n",
    "        inputs,truths = convert_to_tensors(obs,act,rew,don)\n",
    "        torch.save(inputs,save_dir+\"inputs{i}.pt\".format(i=i))\n",
    "        torch.save(truths,save_dir+\"truths{i}.pt\".format(i=i))\n",
    "\n",
    "    \n",
    "\n",
    "# def load_transitions():\n",
    "#     import os.path\n",
    "#     if not os.path.isfile(save_dir + \"observations.npz\"):\n",
    "#         print(\"No saved transitions found\")\n",
    "#         return()\n",
    "#     #its stored as a dict so grab out the single array. and 0th axis should be the number of transitions\n",
    "#     observations = np.load(save_dir + \"observations.npz\")[\"arr_0\"]\n",
    "#     actions = np.load(save_dir + \"actions.npz\")[\"arr_0\"]\n",
    "#     rewards = np.load(save_dir + \"rewards.npz\")[\"arr_0\"]\n",
    "#     dones = np.load(save_dir + \"dones.npz\")[\"arr_0\"]\n",
    "#     assert(len(observations) == len(actions) == len(rewards) == len(dones))\n",
    "#     print(\"loaded transitions of length\", len(observations))\n",
    "    \n",
    "    \n",
    "\n",
    "#     return observations, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state.flatten()).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, tensor([-6.1989e-06], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
    "debug_policy.act(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # load_transitions()\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()[0]\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            add_transition(state, action, reward, terminated or truncated)\n",
    "            if terminated or truncated:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as \n",
    "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        #\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "    save_transitions()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 10,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "pong_policy = Policy(pong_hyperparameters[\"state_space\"], pong_hyperparameters[\"action_space\"], pong_hyperparameters[\"h_size\"]).to(device)\n",
    "pong_optimizer = optim.Adam(pong_policy.parameters(), lr=pong_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n",
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n",
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n",
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n",
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n",
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n",
      "(998, 3, 210, 160)\n",
      "(998, 1, 210, 160)\n",
      "(998,)\n",
      "(998,)\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(pong_policy,\n",
    "                   pong_optimizer,\n",
    "                   pong_hyperparameters[\"n_training_episodes\"], \n",
    "                   pong_hyperparameters[\"max_t\"],\n",
    "                   pong_hyperparameters[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7795\n"
     ]
    }
   ],
   "source": [
    "print(len(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param policy: The Reinforce agent\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    terminated, truncated = False,False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      action, _ = policy.act(state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-21.0, 0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               pong_hyperparameters[\"max_t\"], \n",
    "               pong_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               pong_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
