{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lq9gvYDqA1T",
        "outputId": "e77b1986-f0a1-4ba3-e106-5b5b27b065f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (2.3.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZKjKkrhqA1V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXB_s-ftr8QY",
        "outputId": "2f115f3c-d581-4f27-fb6f-1607ae005bab"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sqvvOrV3qA1W"
      },
      "outputs": [],
      "source": [
        "save_dir = \"diffusion_training_data/\"\n",
        "#save_dir = '/content/drive/MyDrive/ai ai pong/'+ save_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting conditional_diffusion\n",
            "  Cloning https://github.com/stevenharperja/conditional_diffusion.git to c:\\users\\shwes\\appdata\\local\\temp\\pip-install-r6rmhifa\\conditional-diffusion_e6e4647c4963400ba8b01cf5e8ccb435\n",
            "  Resolved https://github.com/stevenharperja/conditional_diffusion.git to commit ce480bac794a41df4a5e88f5d7e04dba876b7fc4\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (3.8.4)\n",
            "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (0.18.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (4.66.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (10.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tqdm->conditional_diffusion) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->conditional_diffusion) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->conditional_diffusion) (2021.12.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->conditional_diffusion) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch->conditional_diffusion) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch->conditional_diffusion) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/stevenharperja/conditional_diffusion.git 'C:\\Users\\shwes\\AppData\\Local\\Temp\\pip-install-r6rmhifa\\conditional-diffusion_e6e4647c4963400ba8b01cf5e8ccb435'\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchvision\n",
        "# !pip install tensorboard\n",
        "# !pip uninstall conditional_diffusion\n",
        "\n",
        "#install torch with cuda\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install git+https://github.com/stevenharperja/conditional_diffusion.git#egg=conditional_diffusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.16.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (1.63.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (5.26.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (65.5.0)\n",
            "Requirement already satisfied: six>1.9 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard\n",
        "import conditional_diffusion.modules as modules\n",
        "from conditional_diffusion.ddpm_conditional import Diffusion as Diffusion\n",
        "import conditional_diffusion.utils as utils\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FR9B9rn3qA1Z"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "class Net(nn.Module):\n",
        "    def __init__(self, device, embedding_scale = 0):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_scale = embedding_scale\n",
        "\n",
        "        resnet = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        resnet.fc = nn.Identity()\n",
        "        #freeze the weights for resnet\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        ###init and modify diffusion model\n",
        "        diffusion_model = modules.UNet_conditional(num_classes=10).to(device)\n",
        "        ckpt = torch.load(\"models/conditional_ema_ckpt.pt\", map_location=device)\n",
        "        diffusion_model.load_state_dict(ckpt)\n",
        "        #remove nn.embedder and just send an embedding instead\n",
        "        diffusion_model.label_emb = nn.Identity()\n",
        "        #freeze the weights for diffusion model\n",
        "        for param in diffusion_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        #unfreeze all of the diffusion model's decoder layers\n",
        "        # diffusion_model.up1.requires_grad = True\n",
        "        # diffusion_model.sa4.requires_grad = True\n",
        "        # diffusion_model.up2.requires_grad = True\n",
        "        # diffusion_model.sa5.requires_grad = True\n",
        "        diffusion_model.up3.requires_grad = True #unfreeze just the last 3 layers of diffusion model\n",
        "        diffusion_model.sa6.requires_grad = True\n",
        "        diffusion_model.outc.requires_grad = True\n",
        "\n",
        "        ###init upscaler model\n",
        "        upscaler = nn.Sequential(\n",
        "            nn.Upsample(size=(224,224), mode=\"bilinear\", align_corners=True),\n",
        "        )\n",
        "\n",
        "        #input of (N, 3, 224, 224), output of (N, 256)\n",
        "        self.encoder = nn.Sequential(\n",
        "            resnet,\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0), #Using a layer which isnt trainable so I can reduce the size while keeping it frozen.\n",
        "        )\n",
        "        \n",
        "        self.diffusion_model = diffusion_model #input of (N,256) output of (N,3,64,64)\n",
        "        self.upscaler = upscaler\n",
        "        self.final_layer = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1, stride=1, padding=0) # reduce channel size to 1 for black and white\n",
        "        #input of (N, 256), output of (N, 1)\n",
        "        self.reward_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "        )\n",
        "        #input of (N, 256), output of (N, 1)\n",
        "        self.done_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "        )\n",
        "\n",
        "\n",
        "        ###variables for the diffusion model\n",
        "        self.noise_steps = 1000\n",
        "        self.beta_start = 1e-4\n",
        "        self.beta_end = 0.02\n",
        "        self.beta = torch.linspace(self.beta_start, self.beta_end, self.noise_steps).to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "        self.device = device\n",
        "        self.zero_embedding = torch.zeros((1,256)).to(self.device) \n",
        "\n",
        "\n",
        "    #prediction function. this may not work for training. but maybe since its pretrained it might? work\n",
        "    def diffusion_sample(self, embedding): # see https://github.com/stevenharperja/conditional_diffusion/blob/main/conditional_diffusion/ddpm_conditional.py#L41\n",
        "        n = embedding.size()[0]\n",
        "        x = torch.randn((n, 3, 64, 64)).to(self.device)\n",
        "        for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "            t = (torch.ones(n) * i).long().to(self.device)\n",
        "            predicted_noise = self.diffusion_model(x, t, embedding)\n",
        "            if self.embedding_scale > 0:\n",
        "                uncond_predicted_noise = self.diffusion_model(x, t, None)\n",
        "                predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, self.embedding_scale)\n",
        "            alpha = self.alpha[t][:, None, None, None]\n",
        "            alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "            beta = self.beta[t][:, None, None, None]\n",
        "            if i > 1:\n",
        "                noise = torch.randn_like(x)\n",
        "            else:\n",
        "                noise = torch.zeros_like(x)\n",
        "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, t = None, noised_truth = None, use_embedding = True):\n",
        "        \"\"\"\n",
        "        \n",
        "        x: The input image (n,3,224,224)\n",
        "        t: The timesteps vector (n)\n",
        "        noised_truth: a noised version of the target image, used for training (n,3,64,64)\n",
        "        use_embedding: Whether to use the embedding or not, useful for training faster?\n",
        "\n",
        "        \n",
        "        returns:\n",
        "        if training:\n",
        "            it returns a predicted noise, and an upscaled predicted noise, along with a predicted reward and done\n",
        "        if not training\n",
        "            it returns a predicted image, along with a reward and done.\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        embedding = self.zero_embedding.repeat((x.size()[0],1))\n",
        "        if (not self.training) or use_embedding:\n",
        "            embedding = self.encoder(x) #(n,256)       \n",
        "            \n",
        "        unscaled_image = None\n",
        "        if self.training:\n",
        "            if noised_truth == None:\n",
        "                raise Exception(\"Cannot train without a provided noised target image\")\n",
        "            if t == None:\n",
        "                raise Exception(\"Cannot train without a provided timesteps vector\")\n",
        "            unscaled_image = self.diffusion_model(noised_truth,t,embedding) #(n,3,64,64)\n",
        "        else:\n",
        "            unscaled_image = self.diffusion_sample(embedding) #(n,3,64,64) \n",
        "\n",
        "        image = self.upscaler(unscaled_image)\n",
        "        image = self.final_layer(image)#(n,1,224,224)\n",
        "        #if not self.train:\n",
        "        image = (image.clamp(-1, 1) + 1) / 2\n",
        "        image = (image * 255).type(torch.uint8)\n",
        "        rew = self.done_maker(embedding) #(n,1)\n",
        "        don = self.reward_maker(embedding) #(n,1)\n",
        "        if self.training:\n",
        "            return unscaled_image,image,rew,don\n",
        "        else:\n",
        "            return image,rew,don\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of the network\n",
        "net = Net(device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "class PongDataset(Dataset):\n",
        "    def __init__(self, dir, device):\n",
        "        self.dir = dir\n",
        "        self.device = device\n",
        "    def __len__(self):\n",
        "        existing_files = [f for f in listdir(self.dir) if isfile(join(self.dir, f))]\n",
        "        if existing_files:\n",
        "            #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
        "            i = max(*[int(''.join([i for i in f if i.isdigit()])) for f in existing_files])\n",
        "            return i\n",
        "        else:\n",
        "            return 0\n",
        "    def __getitem__(self, index):\n",
        "        # transitions = np.load(save_dir + \"transitions{i}.npz\".format(index))\n",
        "        # observations = transitions[\"observations\"]\n",
        "        # actions = transitions[\"actions\"]\n",
        "        # rewards = transitions[\"rewards\"]\n",
        "        # dones = transitions[\"dones\"]\n",
        "\n",
        "        #use data which was preformatted for the 'trivial' model\n",
        "        input = torch.load(self.dir+\"input{i}.pt\".format(i=index)).to(self.device)\n",
        "        truth = torch.load(self.dir+\"truth{i}.pt\".format(i=index))\n",
        "        truth = tuple([t.to(self.device)for t in truth])\n",
        "\n",
        "        return input, truth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0xNQBUhaqA1a"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "trainset = PongDataset(save_dir,device)\n",
        "if __name__ == '__main__':\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)#, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V2Mqom-GqA1Z"
      },
      "outputs": [],
      "source": [
        "#see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "# define loss function and optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "small_img_criterion = nn.MSELoss()\n",
        "img_criterion = nn.MSELoss()\n",
        "rew_criterion = nn.MSELoss()\n",
        "don_criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "image_importance = 10 #hyperparameter for weighting how important the image is in the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "beta_start=1e-4\n",
        "beta_end=0.02\n",
        "noise_steps = 1000\n",
        "beta = torch.linspace(beta_start, beta_end, noise_steps).to(device)\n",
        "alpha = 1. - beta\n",
        "alpha_hat = torch.cumprod(alpha, dim=0)\n",
        "def noise_given_noise(x,t,noise):\n",
        "    sqrt_alpha_hat = torch.sqrt(alpha_hat[t])[:, None, None, None]\n",
        "    sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t])[:, None, None, None]\n",
        "    Ɛ = noise\n",
        "    return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
        "\n",
        "\n",
        "small_transform = torchvision.transforms.Resize((64, 64))\n",
        "big_transform = torchvision.transforms.Resize((224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install ipywidgets\n",
        "# %pip install jupyter_contrib_nbextensions\n",
        "# !jupyter contrib nbextension install --user\n",
        "# !jupyter nbextension enable --py widgetsnbextension # removed !pip on the recommendation of a comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "<>:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "C:\\Users\\shwes\\AppData\\Local\\Temp\\ipykernel_21284\\1031405801.py:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert(small_images.size() == (batch_size,3,64,64), \"size is \" + str(small_images.size()))\n",
            "12:39:32 - INFO: Starting epoch 0:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fe114fde5ba440fbfbb63f850a8c08c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 224, 224])) that is different to the input size (torch.Size([4, 3, 224, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(trainloader, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#print(i)\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmall_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#64 by 64 image\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[11], line 27\u001b[0m, in \u001b[0;36mPongDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# transitions = np.load(save_dir + \"transitions{i}.npz\".format(index))\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# observations = transitions[\"observations\"]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#use data which was preformatted for the 'trivial' model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;132;43;01m{i}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     28\u001b[0m     truth \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruth\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m=\u001b[39mindex))\n\u001b[0;32m     29\u001b[0m     truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m truth])\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from conditional_diffusion.ddpm_conditional import Diffusion\n",
        "from conditional_diffusion.ddpm_conditional import SummaryWriter\n",
        "from conditional_diffusion.ddpm_conditional import plot_images\n",
        "from conditional_diffusion.ddpm_conditional import save_images  \n",
        "from conditional_diffusion.modules import EMA\n",
        "import os\n",
        "import logging\n",
        "import copy\n",
        "# while len(tqdm._instances) > 0:\n",
        "#     tqdm._instances.pop().close()\n",
        "from tqdm.notebook import tqdm \n",
        "\n",
        "run_name = \"Pong_Generator\"\n",
        "\n",
        "diffusion = Diffusion(img_size=64, device=device)\n",
        "logger = SummaryWriter(os.path.join(\"runs\", run_name))\n",
        "# ema = EMA(0.995)\n",
        "# ema_model = copy.deepcopy(net).eval().requires_grad_(False)\n",
        "l = len(trainloader)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(300):  # loop over the dataset multiple times\n",
        "\n",
        "    logging.info(f\"Starting epoch {epoch}:\")\n",
        "    pbar = tqdm(trainloader, position=0,leave=True, ascii=True)\n",
        "\n",
        "\n",
        "    for i, data in enumerate(pbar, 0):\n",
        "        #print(i)\n",
        "        input, truth = data\n",
        "        small_images = truth[0] #64 by 64 image\n",
        "        #images = truth[1] #224 by 224 image\n",
        "        assert(small_images.size() == (batch_size,3,64,64), \"size is \" + str(small_images.size()))\n",
        "\n",
        "\n",
        "        \n",
        "        t = diffusion.sample_timesteps(small_images.shape[0]).to(device) #get <batch size> number of t's\n",
        "        x_t, small_noise = diffusion.noise_images(small_images, t) #TODO change the random noise to not be reallocated in memory so that the time complexity is less.\n",
        "\n",
        "        noise = big_transform(small_noise)#sadly i dont think this can be quickened\n",
        "\n",
        "        use_embedding = True\n",
        "        if np.random.random() < 0.1: #randomly dont use embedding to generate an image.\n",
        "            use_embedding = False\n",
        "        #forward\n",
        "        small_predicted_noise, predicted_noise, rew, don = net(input, t = t, noised_truth = x_t, use_embedding = use_embedding)\n",
        "\n",
        "        loss0 = small_img_criterion(small_noise,small_predicted_noise)\n",
        "        loss1 = img_criterion(noise,predicted_noise)\n",
        "        loss2 = rew_criterion(rew,truth[2])\n",
        "        loss3 = don_criterion(don,truth[3])\n",
        "        loss = loss0 + loss1 + loss2 + loss3\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # ema.step_ema(ema_model, model)\n",
        "\n",
        "        pbar.set_postfix(MSE=loss.item())\n",
        "        logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        net.eval()\n",
        "        labels = torch.arange(10).long().to(device)\n",
        "        sampled_images = net(input[0].unsqueeze(0))[0]\n",
        "        # ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n",
        "        plot_images(sampled_images)\n",
        "        os.makedirs(\"results/{}\".format(run_name), exist_ok = True)\n",
        "        save_images(sampled_images, os.path.join(\"results\", run_name, f\"{epoch}.jpg\"))\n",
        "        # save_images(ema_sampled_images, os.path.join(\"results\", run_name, f\"{epoch}_ema.jpg\"))\n",
        "        os.makedirs(\"models/{}\".format(run_name), exist_ok = True)\n",
        "        torch.save(net.state_dict(), os.path.join(\"models\", run_name, f\"ckpt.pt\"))\n",
        "        # torch.save(ema_model.state_dict(), os.path.join(\"models\", run_name, f\"ema_ckpt.pt\"))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(\"models\", run_name, f\"optim.pt\"))\n",
        "        net.train()\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgRJ_RNBqA1b"
      },
      "outputs": [],
      "source": [
        "PATH = './models/pong_gen.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkxLU375qA1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(hidden_channels=hidden_channels)\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWKouzwLqA1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [00:20, 48.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 224, 224])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input = None\n",
        "net.eval()\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    print(i)\n",
        "    input, truth = data\n",
        "    break\n",
        "\n",
        "output = net(input[0].unsqueeze(0))[0]\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3qb7meBqA1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0 255 255 ... 194 255   0]\n",
            " [255 144   0 ... 194   0 164]\n",
            " [  0   0   0 ...   0   0 255]\n",
            " ...\n",
            " [194 194   0 ... 121   0 151]\n",
            " [  0   0   0 ...   0   0 255]\n",
            " [  0 255 255 ... 255 255   0]]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "image = Image.fromarray(input[0][0].cpu().detach().numpy())# true value\n",
        "image.show()\n",
        "\n",
        "image = Image.fromarray(output[0][0].cpu().detach().numpy())# predicted value\n",
        "image.show()\n",
        "print(output[0][0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u9O5pKEqA1d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'observations' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gif \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(observation) \u001b[38;5;28;01mfor\u001b[39;00m observation \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobservations\u001b[49m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m      4\u001b[0m     gif\u001b[38;5;241m.\u001b[39mappend(image)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'observations' is not defined"
          ]
        }
      ],
      "source": [
        "gif = []\n",
        "images = [Image.fromarray(observation) for observation in observations]\n",
        "for image in images:\n",
        "    gif.append(image)\n",
        "gif[0].save('temp/result.gif', save_all=True,optimize=False, append_images=gif[1:], loop=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
