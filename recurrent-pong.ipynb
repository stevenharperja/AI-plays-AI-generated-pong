{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lq9gvYDqA1T",
        "outputId": "e77b1986-f0a1-4ba3-e106-5b5b27b065f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.18.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (2.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in ./.venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZKjKkrhqA1V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXB_s-ftr8QY",
        "outputId": "2f115f3c-d581-4f27-fb6f-1607ae005bab"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sqvvOrV3qA1W"
      },
      "outputs": [],
      "source": [
        "save_dir = \"diffusion_training_data/\"\n",
        "#save_dir = '/content/drive/MyDrive/ai ai pong/'+ save_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet18\n",
        "encoder = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "# #remove the fully connected layers\n",
        "encoder.fc = nn.Identity()\n",
        "\n",
        "#test it\n",
        "encoder.eval()\n",
        "x = torch.randn(1, 3, 224, 224) # will need to convert pong images from 160 and 210 to this size of 224,224 later.\n",
        "with torch.no_grad():\n",
        "    y = encoder(x)\n",
        "    print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata (from diffusers)\n",
            "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (3.13.3)\n",
            "Collecting huggingface-hub>=0.20.2 (from diffusers)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (1.26.4)\n",
            "Collecting regex!=2019.12.17 (from diffusers)\n",
            "  Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (2.31.0)\n",
            "Collecting safetensors>=0.3.1 (from diffusers)\n",
            "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from diffusers) (10.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.10.0)\n",
            "Collecting zipp>=0.5 (from importlib-metadata->diffusers)\n",
            "  Downloading zipp-3.18.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->diffusers) (2024.2.2)\n",
            "Downloading diffusers-0.27.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading zipp-3.18.1-py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: zipp, tqdm, safetensors, regex, importlib-metadata, huggingface-hub, tokenizers, diffusers, transformers\n",
            "Successfully installed diffusers-0.27.2 huggingface-hub-0.23.0 importlib-metadata-7.1.0 regex-2024.4.28 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.40.2 zipp-3.18.1\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting conditional_diffusion\n",
            "  Cloning https://github.com/stevenharperja/conditional_diffusion.git to c:\\users\\shwes\\appdata\\local\\temp\\pip-install-87xbmn9y\\conditional-diffusion_7d2e550f9e564440a071f1a5277ce0dc\n",
            "  Resolved https://github.com/stevenharperja/conditional_diffusion.git to commit ce480bac794a41df4a5e88f5d7e04dba876b7fc4\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (3.8.4)\n",
            "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (0.18.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (4.66.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (10.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tqdm->conditional_diffusion) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->conditional_diffusion) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->conditional_diffusion) (2021.12.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->conditional_diffusion) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch->conditional_diffusion) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch->conditional_diffusion) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/stevenharperja/conditional_diffusion.git 'C:\\Users\\shwes\\AppData\\Local\\Temp\\pip-install-87xbmn9y\\conditional-diffusion_7d2e550f9e564440a071f1a5277ce0dc'\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchvision\n",
        "# !pip install tensorboard\n",
        "# !pip uninstall conditional_diffusion\n",
        "\n",
        "#install torch with cuda\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install git+https://github.com/stevenharperja/conditional_diffusion.git#egg=conditional_diffusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09:42:30 - INFO: Sampling 1 new images....\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet_conditional(\n",
            "  (inc): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (down1): Down(\n",
            "    (maxpool_conv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa1): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (down2): Down(\n",
            "    (maxpool_conv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa2): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (down3): Down(\n",
            "    (maxpool_conv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa3): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (bot1): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (bot2): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (bot3): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (up1): Up(\n",
            "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (conv): Sequential(\n",
            "      (0): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa4): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (up2): Up(\n",
            "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (conv): Sequential(\n",
            "      (0): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa5): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (up3): Up(\n",
            "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (conv): Sequential(\n",
            "      (0): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa6): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (outc): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (label_emb): Embedding(10, 256)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "301it [00:11, 27.29it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m*\u001b[39m n)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#X is image\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_images(x)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\conditional_diffusion\\ddpm_conditional.py:48\u001b[0m, in \u001b[0;36mDiffusion.sample\u001b[1;34m(self, model, n, labels, cfg_scale)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_steps)), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     47\u001b[0m     t \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mones(n) \u001b[38;5;241m*\u001b[39m i)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 48\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg_scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     50\u001b[0m         uncond_predicted_noise \u001b[38;5;241m=\u001b[39m model(x, t, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\conditional_diffusion\\modules.py:240\u001b[0m, in \u001b[0;36mUNet_conditional.forward\u001b[1;34m(self, x, t, y)\u001b[0m\n\u001b[0;32m    238\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(x3)\n\u001b[0;32m    239\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown3(x3, t)\n\u001b[1;32m--> 240\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot1(x4)\n\u001b[0;32m    243\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot2(x4)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\conditional_diffusion\\modules.py:54\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m attention_value, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(x_ln, x_ln, x_ln)\n\u001b[0;32m     53\u001b[0m attention_value \u001b[38;5;241m=\u001b[39m attention_value \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m---> 54\u001b[0m attention_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_self\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_value\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m attention_value\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_value\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#import modules #dont need it.\n",
        "import conditional_diffusion.modules as modules\n",
        "from conditional_diffusion.ddpm_conditional import Diffusion as Diffusion\n",
        "import conditional_diffusion.utils as utils\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n = 1\n",
        "\n",
        "model = modules.UNet_conditional(num_classes=10).to(device)\n",
        "ckpt = torch.load(\"models/conditional_ema_ckpt.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt)\n",
        "diffusion = Diffusion(img_size=64, device=device)\n",
        "print(model)\n",
        "#Y is embedding\n",
        "y = torch.Tensor([6] * n).long().to(device)\n",
        "#X is image\n",
        "x = diffusion.sample(model, n, y, cfg_scale=3)\n",
        "utils.plot_images(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FR9B9rn3qA1Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        resnet = resnet50(weights=\"IMAGENET1K_V2\").fc = nn.Identity()\n",
        "        #freeze the weights for resnet\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        #input of (N, 3, 224, 224), output of (N, 2048)\n",
        "        self.encoder = resnet\n",
        "        #input of (N, ????), output of (N, 1, 256, 256)\n",
        "        self.generator = nn.Sequential(\n",
        "            #<diffusion model>, # placeholder for diffusion model\n",
        "            nn.conv2d(in_channels=3, out_channels=1, kernel_size=1, stride=1, padding=0), # reduce channel size to 1 for black and white\n",
        "        )\n",
        "        #input of (N, 2048), output of (N, 1)\n",
        "        self.reward_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=1),\n",
        "        )\n",
        "        #input of (N, 2048), output of (N, 1)\n",
        "        self.done_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.encoder(x)\n",
        "        image = self.generator(embedding) #(n,1,256,256)\n",
        "        rew = self.done_maker(embedding) #(n,1)\n",
        "        don = self.reward_maker(embedding) #(n,1)\n",
        "        return image,rew,don\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of the network\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V2Mqom-GqA1Z"
      },
      "outputs": [],
      "source": [
        "#see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "# define loss function and optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "img_criterion = nn.MSELoss()\n",
        "rew_criterion = nn.MSELoss()\n",
        "don_criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "image_importance = 10 #hyperparameter for weighting how important the image is in the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "class PongDataset(Dataset):\n",
        "    def __init__(self, dir, device):\n",
        "        self.dir = dir\n",
        "        self.device = device\n",
        "    def __len__(self):\n",
        "        existing_files = [f for f in listdir(self.dir) if isfile(join(self.dir, f))]\n",
        "        if existing_files:\n",
        "            #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
        "            i = max(*[int(''.join([i for i in f if i.isdigit()])) for f in existing_files])\n",
        "            return i\n",
        "        else:\n",
        "            return 0\n",
        "    def __getitem__(self, index):\n",
        "        # transitions = np.load(save_dir + \"transitions{i}.npz\".format(index))\n",
        "        # observations = transitions[\"observations\"]\n",
        "        # actions = transitions[\"actions\"]\n",
        "        # rewards = transitions[\"rewards\"]\n",
        "        # dones = transitions[\"dones\"]\n",
        "\n",
        "        #use data which was preformatted for the 'trivial' model\n",
        "        input = torch.load(self.dir+\"input{i}.pt\".format(i=index)).to(self.device)\n",
        "        truth = torch.load(self.dir+\"truth{i}.pt\".format(i=index))\n",
        "        truth = tuple([t.to(self.device)for t in truth])\n",
        "\n",
        "        return input, truth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0xNQBUhaqA1a"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "trainset = PongDataset(save_dir,device)\n",
        "if __name__ == '__main__':\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)#, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shwes\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([4, 1])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "Finished Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shwes\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([3, 1])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        print(i)\n",
        "        input, truth = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        img,r,d = net(input)\n",
        "        loss1 = img_criterion(img,truth[0])\n",
        "        loss2 = rew_criterion(r,truth[1])\n",
        "        loss3 = don_criterion(d,truth[2])\n",
        "        loss = loss1#(image_importance * loss1) + loss2 + loss3\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KgRJ_RNBqA1b"
      },
      "outputs": [],
      "source": [
        "PATH = './models/pong_gen.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vkxLU375qA1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(hidden_channels=hidden_channels)\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MWKouzwLqA1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 210, 160])\n"
          ]
        }
      ],
      "source": [
        "output = net(input[0].unsqueeze(0))[0]\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "H3qb7meBqA1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "image = Image.fromarray(input[0][0].cpu().detach().numpy())# true value\n",
        "image.show()\n",
        "\n",
        "image = Image.fromarray(output[0][0].cpu().detach().numpy())# predicted value\n",
        "image.show()\n",
        "print(output[0][0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u9O5pKEqA1d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'observations' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gif \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(observation) \u001b[38;5;28;01mfor\u001b[39;00m observation \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobservations\u001b[49m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m      4\u001b[0m     gif\u001b[38;5;241m.\u001b[39mappend(image)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'observations' is not defined"
          ]
        }
      ],
      "source": [
        "gif = []\n",
        "images = [Image.fromarray(observation) for observation in observations]\n",
        "for image in images:\n",
        "    gif.append(image)\n",
        "gif[0].save('temp/result.gif', save_all=True,optimize=False, append_images=gif[1:], loop=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
