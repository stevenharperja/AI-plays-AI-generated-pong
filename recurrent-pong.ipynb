{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lq9gvYDqA1T",
        "outputId": "e77b1986-f0a1-4ba3-e106-5b5b27b065f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (2.3.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZKjKkrhqA1V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXB_s-ftr8QY",
        "outputId": "2f115f3c-d581-4f27-fb6f-1607ae005bab"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sqvvOrV3qA1W"
      },
      "outputs": [],
      "source": [
        "save_dir = \"diffusion_training_data/\"\n",
        "#save_dir = '/content/drive/MyDrive/ai ai pong/'+ save_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting conditional_diffusion\n",
            "  Cloning https://github.com/stevenharperja/conditional_diffusion.git to c:\\users\\shwes\\appdata\\local\\temp\\pip-install-17eq1i3p\\conditional-diffusion_45e482d2a2f64cb09c5d75825c4f279c\n",
            "  Resolved https://github.com/stevenharperja/conditional_diffusion.git to commit ce480bac794a41df4a5e88f5d7e04dba876b7fc4\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (3.8.4)\n",
            "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (0.18.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (4.66.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from conditional_diffusion) (10.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from matplotlib->conditional_diffusion) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch->conditional_diffusion) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tqdm->conditional_diffusion) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->conditional_diffusion) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->conditional_diffusion) (2021.12.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->conditional_diffusion) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch->conditional_diffusion) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch->conditional_diffusion) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/stevenharperja/conditional_diffusion.git 'C:\\Users\\shwes\\AppData\\Local\\Temp\\pip-install-17eq1i3p\\conditional-diffusion_45e482d2a2f64cb09c5d75825c4f279c'\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchvision\n",
        "# !pip install tensorboard\n",
        "# !pip uninstall conditional_diffusion\n",
        "\n",
        "#install torch with cuda\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install git+https://github.com/stevenharperja/conditional_diffusion.git#egg=conditional_diffusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.16.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (1.63.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (5.26.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (65.5.0)\n",
            "Requirement already satisfied: six>1.9 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard\n",
        "import conditional_diffusion.modules as modules\n",
        "from conditional_diffusion.ddpm_conditional import Diffusion as Diffusion\n",
        "import conditional_diffusion.utils as utils\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FR9B9rn3qA1Z"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "class Net(nn.Module):\n",
        "    def __init__(self, device, embedding_scale = 0):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_scale = embedding_scale\n",
        "\n",
        "        resnet = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        resnet.fc = nn.Identity()\n",
        "        #freeze the weights for resnet\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        ###init and modify diffusion model\n",
        "        diffusion_model = modules.UNet_conditional(num_classes=10).to(device)\n",
        "        ckpt = torch.load(\"models/conditional_ema_ckpt.pt\", map_location=device)\n",
        "        diffusion_model.load_state_dict(ckpt)\n",
        "        #remove nn.embedder and just send an embedding instead\n",
        "        diffusion_model.label_emb = nn.Identity()\n",
        "        # #freeze the weights for diffusion model\n",
        "        # for param in diffusion_model.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        # #unfreeze all of the diffusion model's decoder layers\n",
        "        # diffusion_model.up1.requires_grad = True\n",
        "        # diffusion_model.sa4.requires_grad = True\n",
        "        # diffusion_model.up2.requires_grad = True\n",
        "        # diffusion_model.sa5.requires_grad = True\n",
        "        # diffusion_model.up3.requires_grad = True\n",
        "        # diffusion_model.sa6.requires_grad = True\n",
        "        # diffusion_model.outc.requires_grad = True\n",
        "\n",
        "        ###init upscaler model\n",
        "        upscaler = nn.Sequential(\n",
        "            nn.Upsample(size=(224,224), mode=\"bilinear\", align_corners=True),\n",
        "            modules.DoubleConv(in_channels=3,out_channels=3,residual=True),\n",
        "            modules.DoubleConv(in_channels=3,out_channels=3,residual=True),\n",
        "        )\n",
        "\n",
        "        #input of (N, 3, 224, 224), output of (N, 256)\n",
        "        self.encoder = nn.Sequential(\n",
        "            resnet,\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0), #Using a layer which isnt trainable so I can reduce the size while keeping it frozen.\n",
        "        )\n",
        "        \n",
        "        self.diffusion_model = diffusion_model #input of (N,256) output of (N,3,64,64)\n",
        "        self.upscaler = upscaler\n",
        "        self.final_layer = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1, stride=1, padding=0) # reduce channel size to 1 for black and white\n",
        "        #input of (N, 256), output of (N, 1)\n",
        "        self.reward_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "        )\n",
        "        #input of (N, 256), output of (N, 1)\n",
        "        self.done_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "        )\n",
        "\n",
        "\n",
        "        ###variables for the diffusion model\n",
        "        self.noise_steps = 1000\n",
        "        self.beta_start = 1e-4\n",
        "        self.beta_end = 0.02\n",
        "        self.beta = torch.linspace(self.beta_start, self.beta_end, self.noise_steps).to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    #prediction function. this may not work for training. but maybe since its pretrained it might? work\n",
        "    def diffusion_sample(self, embedding): # see https://github.com/stevenharperja/conditional_diffusion/blob/main/conditional_diffusion/ddpm_conditional.py#L41\n",
        "        n = embedding.size()[0]\n",
        "        x = torch.randn((n, 3, 64, 64)).to(self.device)\n",
        "        for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "            t = (torch.ones(n) * i).long().to(self.device)\n",
        "            predicted_noise = self.diffusion_model(x, t, embedding)\n",
        "            if self.embedding_scale > 0:\n",
        "                uncond_predicted_noise = self.diffusion_model(x, t, None)\n",
        "                predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, self.embedding_scale)\n",
        "            alpha = self.alpha[t][:, None, None, None]\n",
        "            alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "            beta = self.beta[t][:, None, None, None]\n",
        "            if i > 1:\n",
        "                noise = torch.randn_like(x)\n",
        "            else:\n",
        "                noise = torch.zeros_like(x)\n",
        "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, t = None, noised_truth = None, use_embedding = True):\n",
        "        \"\"\"\n",
        "        \n",
        "        x: The input image (n,3,224,224)\n",
        "        t: The timesteps vector (n)\n",
        "        noised_truth: a noised version of the target image, used for training (n,3,64,64)\n",
        "        use_embedding: Whether to use the embedding or not, useful for training faster?\n",
        "\n",
        "        \n",
        "        returns:\n",
        "        if training:\n",
        "            it returns a predicted noise, and an upscaled predicted noise, along with a predicted reward and done\n",
        "        if not training\n",
        "            it returns a predicted image, along with a reward and done.\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        embedding = None\n",
        "        if (not self.training) or use_embedding:\n",
        "            embedding = self.encoder(x) #(n,256)        \n",
        "        unscaled_image = None\n",
        "        if self.training:\n",
        "            if noised_truth == None:\n",
        "                raise Exception(\"Cannot train without a provided noised target image\")\n",
        "            if t == None:\n",
        "                raise Exception(\"Cannot train without a provided timesteps vector\")\n",
        "            unscaled_image = self.diffusion_model(noised_truth,t,embedding) #(n,3,64,64)\n",
        "        else:\n",
        "            unscaled_image = self.diffusion_sample(embedding) #(n,3,64,64) \n",
        "\n",
        "        image = self.upscaler(unscaled_image)\n",
        "        image = self.final_layer(image)#(n,1,224,224)\n",
        "        #if not self.train:\n",
        "        image = (image.clamp(-1, 1) + 1) / 2\n",
        "        image = (image * 255).type(torch.uint8)\n",
        "        rew = self.done_maker(embedding) #(n,1)\n",
        "        don = self.reward_maker(embedding) #(n,1)\n",
        "        if self.training:\n",
        "            return unscaled_image,image,rew,don\n",
        "        else:\n",
        "            return image,rew,don\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of the network\n",
        "net = Net(device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "class PongDataset(Dataset):\n",
        "    def __init__(self, dir, device):\n",
        "        self.dir = dir\n",
        "        self.device = device\n",
        "    def __len__(self):\n",
        "        existing_files = [f for f in listdir(self.dir) if isfile(join(self.dir, f))]\n",
        "        if existing_files:\n",
        "            #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
        "            i = max(*[int(''.join([i for i in f if i.isdigit()])) for f in existing_files])\n",
        "            return i\n",
        "        else:\n",
        "            return 0\n",
        "    def __getitem__(self, index):\n",
        "        # transitions = np.load(save_dir + \"transitions{i}.npz\".format(index))\n",
        "        # observations = transitions[\"observations\"]\n",
        "        # actions = transitions[\"actions\"]\n",
        "        # rewards = transitions[\"rewards\"]\n",
        "        # dones = transitions[\"dones\"]\n",
        "\n",
        "        #use data which was preformatted for the 'trivial' model\n",
        "        input = torch.load(self.dir+\"input{i}.pt\".format(i=index)).to(self.device)\n",
        "        truth = torch.load(self.dir+\"truth{i}.pt\".format(i=index))\n",
        "        truth = tuple([t.to(self.device)for t in truth])\n",
        "\n",
        "        return input, truth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0xNQBUhaqA1a"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "trainset = PongDataset(save_dir,device)\n",
        "if __name__ == '__main__':\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)#, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V2Mqom-GqA1Z"
      },
      "outputs": [],
      "source": [
        "#see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "# define loss function and optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "small_img_criterion = nn.MSELoss()\n",
        "img_criterion = nn.MSELoss()\n",
        "rew_criterion = nn.MSELoss()\n",
        "don_criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "image_importance = 10 #hyperparameter for weighting how important the image is in the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "beta_start=1e-4\n",
        "beta_end=0.02\n",
        "noise_steps = 1000\n",
        "beta = torch.linspace(beta_start, beta_end, noise_steps).to(device)\n",
        "alpha = 1. - beta\n",
        "alpha_hat = torch.cumprod(alpha, dim=0)\n",
        "def noise_given_noise(x,t,noise):\n",
        "    sqrt_alpha_hat = torch.sqrt(alpha_hat[t])[:, None, None, None]\n",
        "    sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t])[:, None, None, None]\n",
        "    Ɛ = noise\n",
        "    return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
        "\n",
        "\n",
        "small_transform = torchvision.transforms.Resize((64, 64))\n",
        "big_transform = torchvision.transforms.Resize((224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:30: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "<>:30: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "C:\\Users\\shwes\\AppData\\Local\\Temp\\ipykernel_25396\\3220251056.py:30: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert(small_images.size() == (batch_size,3,64,64), \"size is \" + str(small_images.size()))\n",
            "11:00:26 - INFO: Starting epoch 0:\n",
            "  0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 224, 224])) that is different to the input size (torch.Size([4, 3, 224, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "  0%|          | 0/250 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m ema\u001b[38;5;241m.\u001b[39mstep_ema(ema_model, model)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU "
          ]
        }
      ],
      "source": [
        "from conditional_diffusion.ddpm_conditional import Diffusion\n",
        "from conditional_diffusion.ddpm_conditional import SummaryWriter\n",
        "from conditional_diffusion.ddpm_conditional import plot_images\n",
        "from conditional_diffusion.ddpm_conditional import save_images  \n",
        "from conditional_diffusion.modules import EMA\n",
        "import os\n",
        "import logging\n",
        "import copy\n",
        "\n",
        "\n",
        "run_name = \"Pong_Generator\"\n",
        "\n",
        "diffusion = Diffusion(img_size=64, device=device)\n",
        "logger = SummaryWriter(os.path.join(\"runs\", run_name))\n",
        "# ema = EMA(0.995)\n",
        "# ema_model = copy.deepcopy(net).eval().requires_grad_(False)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(300):  # loop over the dataset multiple times\n",
        "\n",
        "    logging.info(f\"Starting epoch {epoch}:\")\n",
        "    pbar = tqdm(trainloader)\n",
        "\n",
        "\n",
        "    for i, data in enumerate(pbar, 0):\n",
        "        print(i)\n",
        "        input, truth = data\n",
        "        small_images = truth[0] #64 by 64 image\n",
        "        #images = truth[1] #224 by 224 image\n",
        "        assert(small_images.size() == (batch_size,3,64,64), \"size is \" + str(small_images.size()))\n",
        "\n",
        "\n",
        "        \n",
        "        t = diffusion.sample_timesteps(small_images.shape[0]).to(device) #get <batch size> number of t's\n",
        "        x_t, small_noise = diffusion.noise_images(small_images, t) #TODO change the random noise to not be reallocated in memory so that the time complexity is less.\n",
        "\n",
        "        noise = big_transform(small_noise)#sadly i dont think this can be quickened\n",
        "\n",
        "        use_embedding = True\n",
        "        if np.random.random() < 0.1: #randomly dont use embedding to generate an image.\n",
        "            use_embedding = False\n",
        "        #forward\n",
        "        small_predicted_noise, predicted_noise, rew, don = net(input, t = t, noised_truth = x_t, use_embedding = use_embedding)\n",
        "\n",
        "        loss0 = small_img_criterion(small_noise,small_predicted_noise)\n",
        "        loss1 = img_criterion(noise,predicted_noise)\n",
        "        loss2 = rew_criterion(rew,truth[2])\n",
        "        loss3 = don_criterion(don,truth[3])\n",
        "        loss = loss0 + loss1 + loss2 + loss3\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # ema.step_ema(ema_model, model)\n",
        "\n",
        "        pbar.set_postfix(MSE=loss.item())\n",
        "        logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        net.eval()\n",
        "        labels = torch.arange(10).long().to(device)\n",
        "        sampled_images = net(input[0].unsqueeze(0))[0]\n",
        "        # ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n",
        "        plot_images(sampled_images)\n",
        "        save_images(sampled_images, os.path.join(\"results\", run_name, f\"{epoch}.jpg\"))\n",
        "        # save_images(ema_sampled_images, os.path.join(\"results\", run_name, f\"{epoch}_ema.jpg\"))\n",
        "        torch.save(net.state_dict(), os.path.join(\"models\", run_name, f\"ckpt.pt\"))\n",
        "        # torch.save(ema_model.state_dict(), os.path.join(\"models\", run_name, f\"ema_ckpt.pt\"))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(\"models\", run_name, f\"optim.pt\"))\n",
        "        net.train()\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KgRJ_RNBqA1b"
      },
      "outputs": [],
      "source": [
        "PATH = './models/pong_gen.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vkxLU375qA1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(hidden_channels=hidden_channels)\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "MWKouzwLqA1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [00:20, 48.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 224, 224])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input = None\n",
        "net.eval()\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    print(i)\n",
        "    input, truth = data\n",
        "    break\n",
        "\n",
        "output = net(input[0].unsqueeze(0))[0]\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "H3qb7meBqA1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0 255 255 ... 194 255   0]\n",
            " [255 144   0 ... 194   0 164]\n",
            " [  0   0   0 ...   0   0 255]\n",
            " ...\n",
            " [194 194   0 ... 121   0 151]\n",
            " [  0   0   0 ...   0   0 255]\n",
            " [  0 255 255 ... 255 255   0]]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "image = Image.fromarray(input[0][0].cpu().detach().numpy())# true value\n",
        "image.show()\n",
        "\n",
        "image = Image.fromarray(output[0][0].cpu().detach().numpy())# predicted value\n",
        "image.show()\n",
        "print(output[0][0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u9O5pKEqA1d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'observations' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gif \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(observation) \u001b[38;5;28;01mfor\u001b[39;00m observation \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobservations\u001b[49m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m      4\u001b[0m     gif\u001b[38;5;241m.\u001b[39mappend(image)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'observations' is not defined"
          ]
        }
      ],
      "source": [
        "gif = []\n",
        "images = [Image.fromarray(observation) for observation in observations]\n",
        "for image in images:\n",
        "    gif.append(image)\n",
        "gif[0].save('temp/result.gif', save_all=True,optimize=False, append_images=gif[1:], loop=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
