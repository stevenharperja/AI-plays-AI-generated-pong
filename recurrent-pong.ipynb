{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lq9gvYDqA1T",
        "outputId": "e77b1986-f0a1-4ba3-e106-5b5b27b065f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.18.0)\n",
            "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torchvision) (2.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (4.10.0)\n",
            "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZKjKkrhqA1V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXB_s-ftr8QY",
        "outputId": "2f115f3c-d581-4f27-fb6f-1607ae005bab"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sqvvOrV3qA1W"
      },
      "outputs": [],
      "source": [
        "save_dir = \"diffusion_training_data/\"\n",
        "#save_dir = '/content/drive/MyDrive/ai ai pong/'+ save_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/codespace/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet18\n",
        "encoder = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "# #remove the fully connected layers\n",
        "encoder.fc = nn.Identity()\n",
        "\n",
        "#test it\n",
        "encoder.eval()\n",
        "x = torch.randn(1, 3, 224, 224) # will need to convert pong images from 160 and 210 to this size of 224,224 later.\n",
        "with torch.no_grad():\n",
        "    y = encoder(x)\n",
        "    print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Collecting conditional_diffusion\n",
            "  Cloning https://github.com/stevenharperja/conditional_diffusion.git to /tmp/pip-install-vfa114x_/conditional-diffusion_41d00cc9b85d42f49c998ef56c0a16cf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stevenharperja/conditional_diffusion.git /tmp/pip-install-vfa114x_/conditional-diffusion_41d00cc9b85d42f49c998ef56c0a16cf\n",
            "  Resolved https://github.com/stevenharperja/conditional_diffusion.git to commit ce480bac794a41df4a5e88f5d7e04dba876b7fc4\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from conditional_diffusion) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.10/site-packages (from conditional_diffusion) (3.8.3)\n",
            "Requirement already satisfied: torch in /usr/local/python/3.10.13/lib/python3.10/site-packages (from conditional_diffusion) (2.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/python/3.10.13/lib/python3.10/site-packages (from conditional_diffusion) (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from conditional_diffusion) (4.66.4)\n",
            "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from conditional_diffusion) (10.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->conditional_diffusion) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (4.10.0)\n",
            "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (1.12)\n",
            "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torch->conditional_diffusion) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch->conditional_diffusion) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from torch->conditional_diffusion) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->conditional_diffusion) (12.4.99)\n",
            "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->conditional_diffusion) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch->conditional_diffusion) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch->conditional_diffusion) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchvision\n",
        "# !pip install tensorboard\n",
        "# !pip uninstall conditional_diffusion\n",
        "\n",
        "#install torch with cuda\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install git+https://github.com/stevenharperja/conditional_diffusion.git#egg=conditional_diffusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "09:42:30 - INFO: Sampling 1 new images....\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet_conditional(\n",
            "  (inc): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (down1): Down(\n",
            "    (maxpool_conv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa1): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (down2): Down(\n",
            "    (maxpool_conv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa2): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (down3): Down(\n",
            "    (maxpool_conv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa3): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (bot1): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (bot2): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (bot3): DoubleConv(\n",
            "    (double_conv): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (up1): Up(\n",
            "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (conv): Sequential(\n",
            "      (0): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa4): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (up2): Up(\n",
            "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (conv): Sequential(\n",
            "      (0): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa5): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (up3): Up(\n",
            "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (conv): Sequential(\n",
            "      (0): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): DoubleConv(\n",
            "        (double_conv): Sequential(\n",
            "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (emb_layer): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (sa6): SelfAttention(\n",
            "    (mha): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    (ff_self): Sequential(\n",
            "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (outc): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (label_emb): Embedding(10, 256)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "301it [00:11, 27.29it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m*\u001b[39m n)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#X is image\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_images(x)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\conditional_diffusion\\ddpm_conditional.py:48\u001b[0m, in \u001b[0;36mDiffusion.sample\u001b[1;34m(self, model, n, labels, cfg_scale)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_steps)), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     47\u001b[0m     t \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mones(n) \u001b[38;5;241m*\u001b[39m i)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 48\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg_scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     50\u001b[0m         uncond_predicted_noise \u001b[38;5;241m=\u001b[39m model(x, t, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\conditional_diffusion\\modules.py:240\u001b[0m, in \u001b[0;36mUNet_conditional.forward\u001b[1;34m(self, x, t, y)\u001b[0m\n\u001b[0;32m    238\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(x3)\n\u001b[0;32m    239\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown3(x3, t)\n\u001b[1;32m--> 240\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot1(x4)\n\u001b[0;32m    243\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot2(x4)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\conditional_diffusion\\modules.py:54\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m attention_value, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(x_ln, x_ln, x_ln)\n\u001b[0;32m     53\u001b[0m attention_value \u001b[38;5;241m=\u001b[39m attention_value \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m---> 54\u001b[0m attention_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_self\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_value\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m attention_value\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_value\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize)\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#import modules #dont need it.\n",
        "import conditional_diffusion.modules as modules\n",
        "from conditional_diffusion.ddpm_conditional import Diffusion as Diffusion\n",
        "import conditional_diffusion.utils as utils\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n = 1\n",
        "\n",
        "model = modules.UNet_conditional(num_classes=10).to(device)\n",
        "ckpt = torch.load(\"models/conditional_ema_ckpt.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt)\n",
        "diffusion = Diffusion(img_size=64, device=device)\n",
        "print(model)\n",
        "#Y is embedding\n",
        "y = torch.Tensor([6] * n).long().to(device)\n",
        "#X is image\n",
        "x = diffusion.sample(model, n, y, cfg_scale=3) #set cfg_scale to 0 for half the lag?\n",
        "utils.plot_images(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## upscaler model from https://github.com/yjn870/SRCNN-pytorch/blob/master/models.py ##UNUSED\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self, num_channels=1):\n",
        "        super(SRCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
        "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upscaler_weights_file = \"./models/srcnn_x4.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([], size=(0, 1, 224, 224), grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "sample_im = torch.ones(0, 1, 64, 64)#.cuda()\n",
        "l1 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=8, bias=False)\n",
        "l2 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=0, bias=False)\n",
        "sample_deconv = nn.Sequential(\n",
        "                l1,\n",
        "                nn.ReLU(),\n",
        "                l2,\n",
        "                nn.ReLU(),\n",
        ")\n",
        "l1.weight = torch.nn.Parameter(\n",
        "    torch.tensor([[[[ 1.,  1.], \n",
        "                    [ 2.,  3.], \n",
        "                    ]]]))#.cuda())\n",
        "l2.weight = torch.nn.Parameter(\n",
        "    torch.tensor([[[[ 1.,  1.], \n",
        "                    [ 2.,  3.], \n",
        "                    ]]]))#.cuda())\n",
        "print(sample_deconv(sample_im))\n",
        "\n",
        "alternate_upscaler =  nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=8, bias=False),\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=0, bias=False),\n",
        "                nn.ReLU(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting absl-py>=0.4 (from tensorboard)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard)\n",
            "  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard)\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
            "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard) (68.2.2)\n",
            "Requirement already satisfied: six>1.9 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
            "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
            "Successfully installed absl-py-2.1.0 grpcio-1.63.0 markdown-3.6 protobuf-5.26.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard\n",
        "import conditional_diffusion.modules as modules\n",
        "from conditional_diffusion.ddpm_conditional import Diffusion as Diffusion\n",
        "import conditional_diffusion.utils as utils\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FR9B9rn3qA1Z"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tqdm'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/AI-plays-AI-generated-pong/recurrent-pong.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bvigilant-succotash-pjx76vwx997fr56r/workspaces/AI-plays-AI-generated-pong/recurrent-pong.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvigilant-succotash-pjx76vwx997fr56r/workspaces/AI-plays-AI-generated-pong/recurrent-pong.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNet\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bvigilant-succotash-pjx76vwx997fr56r/workspaces/AI-plays-AI-generated-pong/recurrent-pong.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, device, embedding_scale \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm \n",
        "class Net(nn.Module):\n",
        "    def __init__(self, device, embedding_scale = 0):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_scale = embedding_scale\n",
        "\n",
        "        resnet = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "        resnet.fc = nn.Identity()\n",
        "        #freeze the weights for resnet\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        ###init and modify diffusion model\n",
        "        diffusion_model = modules.UNet_conditional(num_classes=10).to(device)\n",
        "        ckpt = torch.load(\"models/conditional_ema_ckpt.pt\", map_location=device)\n",
        "        diffusion_model.load_state_dict(ckpt)\n",
        "        #remove nn.embedder and just send an embedding instead\n",
        "        diffusion_model.label_emb = nn.Identity()\n",
        "        # #freeze the weights for diffusion model\n",
        "        # for param in diffusion_model.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        # #unfreeze all of the diffusion model's decoder layers\n",
        "        # diffusion_model.up1.requires_grad = True\n",
        "        # diffusion_model.sa4.requires_grad = True\n",
        "        # diffusion_model.up2.requires_grad = True\n",
        "        # diffusion_model.sa5.requires_grad = True\n",
        "        # diffusion_model.up3.requires_grad = True\n",
        "        # diffusion_model.sa6.requires_grad = True\n",
        "        # diffusion_model.outc.requires_grad = True\n",
        "\n",
        "        ###init upscaler model\n",
        "        upscaler = nn.Sequential(\n",
        "            nn.Upsample(size=(224,224), mode=\"bilinear\", align_corners=True),\n",
        "            modules.DoubleConv(in_channels=3,out_channels=3,residual=True),\n",
        "            modules.DoubleConv(in_channels=3,out_channels=3,residual=True),\n",
        "        )\n",
        "\n",
        "        #input of (N, 3, 224, 224), output of (N, 256)\n",
        "        self.encoder = nn.Sequential(\n",
        "            resnet,\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0), #Using a layer which isnt trainable so I can reduce the size while keeping it frozen.\n",
        "        )\n",
        "        \n",
        "        self.diffusion_model = diffusion_model #input of (N,256) output of (N,3,64,64)\n",
        "        self.upscaler = upscaler\n",
        "        self.final_layer = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1, stride=1, padding=0), # reduce channel size to 1 for black and white\n",
        "        #input of (N, 256), output of (N, 1)\n",
        "        self.reward_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "        )\n",
        "        #input of (N, 256), output of (N, 1)\n",
        "        self.done_maker = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "        )\n",
        "\n",
        "\n",
        "        ###variables for the diffusion model\n",
        "        self.noise_steps = 1000\n",
        "        self.beta_start = 1e-4\n",
        "        self.beta_end = 0.02\n",
        "        self.beta = torch.linspace(self.beta_start, self.beta_end, self.noise_steps).to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    #prediction function. this may not work for training. but maybe since its pretrained it might? work\n",
        "    def diffusion_sample(self, embedding): # see https://github.com/stevenharperja/conditional_diffusion/blob/main/conditional_diffusion/ddpm_conditional.py#L41\n",
        "        n = embedding.size()[0]\n",
        "        x = torch.randn((n, 3, 64, 64)).to(self.device)\n",
        "        for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "            t = (torch.ones(n) * i).long().to(self.device)\n",
        "            predicted_noise = self.diffusion_model(x, t, embedding)\n",
        "            if self.embedding_scale > 0:\n",
        "                uncond_predicted_noise = self.diffusion_model(x, t, None)\n",
        "                predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, self.embedding_scale)\n",
        "            alpha = self.alpha[t][:, None, None, None]\n",
        "            alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "            beta = self.beta[t][:, None, None, None]\n",
        "            if i > 1:\n",
        "                noise = torch.randn_like(x)\n",
        "            else:\n",
        "                noise = torch.zeros_like(x)\n",
        "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, t = None, noised_truth = None, use_embedding = True):\n",
        "        \"\"\"\n",
        "        \n",
        "        x: The input image (n,3,224,224)\n",
        "        t: The timesteps vector (n)\n",
        "        noised_truth: a noised version of the target image, used for training (n,3,64,64)\n",
        "        use_embedding: Whether to use the embedding or not, useful for training faster?\n",
        "\n",
        "        \n",
        "        returns:\n",
        "        if training:\n",
        "            it returns a predicted noise, and an upscaled predicted noise, along with a predicted reward and done\n",
        "        if not training\n",
        "            it returns a predicted image, along with a reward and done.\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        embedding = None\n",
        "        if (not self.training) or use_embedding:\n",
        "            embedding = self.encoder(x) #(n,256)        \n",
        "        unscaled_image = None\n",
        "        if self.training:\n",
        "            if noised_truth == None:\n",
        "                raise Exception(\"Cannot train without a provided noised target image\")\n",
        "            if t == None:\n",
        "                raise Exception(\"Cannot train without a provided timesteps vector\")\n",
        "            unscaled_image = self.diffusion_forward_train(noised_truth,t,embedding) #(n,3,64,64)\n",
        "        else:\n",
        "            unscaled_image = self.diffusion_sample(embedding) #(n,3,64,64) \n",
        "\n",
        "        image = self.upscaler(unscaled_image)\n",
        "        image = self.final_layer(image)#(n,1,224,224)\n",
        "        #if not self.train:\n",
        "        image = (image.clamp(-1, 1) + 1) / 2\n",
        "        image = (image * 255).type(torch.uint8)\n",
        "        rew = self.done_maker(embedding) #(n,1)\n",
        "        don = self.reward_maker(embedding) #(n,1)\n",
        "        if self.training:\n",
        "            return unscaled_image,image,rew,don\n",
        "        else:\n",
        "            return image,rew,don\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of the network\n",
        "net = Net(device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "class PongDataset(Dataset):\n",
        "    def __init__(self, dir, device):\n",
        "        self.dir = dir\n",
        "        self.device = device\n",
        "    def __len__(self):\n",
        "        existing_files = [f for f in listdir(self.dir) if isfile(join(self.dir, f))]\n",
        "        if existing_files:\n",
        "            #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
        "            i = max(*[int(''.join([i for i in f if i.isdigit()])) for f in existing_files])\n",
        "            return i\n",
        "        else:\n",
        "            return 0\n",
        "    def __getitem__(self, index):\n",
        "        # transitions = np.load(save_dir + \"transitions{i}.npz\".format(index))\n",
        "        # observations = transitions[\"observations\"]\n",
        "        # actions = transitions[\"actions\"]\n",
        "        # rewards = transitions[\"rewards\"]\n",
        "        # dones = transitions[\"dones\"]\n",
        "\n",
        "        #use data which was preformatted for the 'trivial' model\n",
        "        input = torch.load(self.dir+\"input{i}.pt\".format(i=index)).to(self.device)\n",
        "        truth = torch.load(self.dir+\"truth{i}.pt\".format(i=index))\n",
        "        truth = tuple([t.to(self.device)for t in truth])\n",
        "\n",
        "        return input, truth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0xNQBUhaqA1a"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "trainset = PongDataset(save_dir,device)\n",
        "if __name__ == '__main__':\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)#, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V2Mqom-GqA1Z"
      },
      "outputs": [],
      "source": [
        "#see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "# define loss function and optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "small_img_criterion = nn.MSELoss()\n",
        "img_criterion = nn.MSELoss()\n",
        "rew_criterion = nn.MSELoss()\n",
        "don_criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "image_importance = 10 #hyperparameter for weighting how important the image is in the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m img,r,d \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m img_criterion(img,truth[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m rew_criterion(r,truth[\u001b[38;5;241m1\u001b[39m])\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[13], line 91\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 91\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(n,3,64,64) #TODO properly feed in 't' and 'rand' in training code\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m#raise NotImplementedError\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_forward(embedding) \u001b[38;5;66;03m#(n,3,64,64) #TODO properly feed in 't' and 'rand' in training code\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[13], line 73\u001b[0m, in \u001b[0;36mNet.diffusion_forward\u001b[0;34m(self, embedding)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_steps)), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     72\u001b[0m     t \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mones(n) \u001b[38;5;241m*\u001b[39m i)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 73\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     75\u001b[0m         uncond_predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model(x, t, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/conditional_diffusion/modules.py:229\u001b[0m, in \u001b[0;36mUNet_conditional.forward\u001b[0;34m(self, x, t, y)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, y):\n\u001b[1;32m    228\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m--> 229\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_emb(y)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/conditional_diffusion/modules.py:220\u001b[0m, in \u001b[0;36mUNet_conditional.pos_encoding\u001b[0;34m(self, t, channels)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, channels):\n\u001b[1;32m    218\u001b[0m     inv_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m channels)\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     pos_enc_a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(t\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m inv_freq)\n\u001b[1;32m    223\u001b[0m     pos_enc_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(t\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m inv_freq)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "from conditional_diffusion.ddpm_conditional import Diffusion\n",
        "from conditional_diffusion.ddpm_conditional import SummaryWriter\n",
        "from conditional_diffusion.ddpm_conditional import plot_images\n",
        "from conditional_diffusion.ddpm_conditional import save_images  \n",
        "from conditional_diffusion.modules import EMA\n",
        "import os\n",
        "import logging\n",
        "\n",
        "diffusion = Diffusion(img_size=224, device=device)\n",
        "logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
        "ema = EMA(0.995)\n",
        "ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(300):  # loop over the dataset multiple times\n",
        "\n",
        "    logging.info(f\"Starting epoch {epoch}:\")\n",
        "    pbar = tqdm(trainloader)\n",
        "\n",
        "\n",
        "    for i, data in enumerate(pbar, 0):\n",
        "        print(i)\n",
        "        input, truth = data\n",
        "        images = images.to(device)\n",
        "        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "        x_t, noise = diffusion.noise_images(images, t)\n",
        "        if np.random.random() < 0.1:\n",
        "            labels = None\n",
        "        predicted_noise = model(x_t, t, labels)\n",
        "\n",
        "\n",
        "        #TODO copy code from conditional diffusion training cycle.\n",
        "        #TODO ^ and for above, change the random noise to not be reallocated in memory so that the time complexity is less.\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        small_img,img,r,d = net(input)\n",
        "        loss0 = small_img_criterion(small_img,truth[0])\n",
        "        loss1 = img_criterion(img,truth[1])\n",
        "        loss2 = rew_criterion(r,truth[2])\n",
        "        loss3 = don_criterion(d,truth[3])\n",
        "        loss = loss0 + loss1 + loss2 + loss3\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ema.step_ema(ema_model, model)\n",
        "\n",
        "        pbar.set_postfix(MSE=loss.item())\n",
        "        logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        labels = torch.arange(10).long().to(device)\n",
        "        sampled_images = diffusion.sample(model, n=len(labels), labels=labels)\n",
        "        ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n",
        "        plot_images(sampled_images)\n",
        "        save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
        "        save_images(ema_sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}_ema.jpg\"))\n",
        "        torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))\n",
        "        torch.save(ema_model.state_dict(), os.path.join(\"models\", args.run_name, f\"ema_ckpt.pt\"))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(\"models\", args.run_name, f\"optim.pt\"))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KgRJ_RNBqA1b"
      },
      "outputs": [],
      "source": [
        "PATH = './models/pong_gen.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vkxLU375qA1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(hidden_channels=hidden_channels)\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "MWKouzwLqA1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [00:20, 48.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 224, 224])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input = None\n",
        "net.eval()\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    print(i)\n",
        "    input, truth = data\n",
        "    break\n",
        "\n",
        "output = net(input[0].unsqueeze(0))[0]\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "H3qb7meBqA1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0 255 255 ... 194 255   0]\n",
            " [255 144   0 ... 194   0 164]\n",
            " [  0   0   0 ...   0   0 255]\n",
            " ...\n",
            " [194 194   0 ... 121   0 151]\n",
            " [  0   0   0 ...   0   0 255]\n",
            " [  0 255 255 ... 255 255   0]]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "image = Image.fromarray(input[0][0].cpu().detach().numpy())# true value\n",
        "image.show()\n",
        "\n",
        "image = Image.fromarray(output[0][0].cpu().detach().numpy())# predicted value\n",
        "image.show()\n",
        "print(output[0][0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u9O5pKEqA1d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'observations' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gif \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(observation) \u001b[38;5;28;01mfor\u001b[39;00m observation \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobservations\u001b[49m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m      4\u001b[0m     gif\u001b[38;5;241m.\u001b[39mappend(image)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'observations' is not defined"
          ]
        }
      ],
      "source": [
        "gif = []\n",
        "images = [Image.fromarray(observation) for observation in observations]\n",
        "for image in images:\n",
        "    gif.append(image)\n",
        "gif[0].save('temp/result.gif', save_all=True,optimize=False, append_images=gif[1:], loop=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
