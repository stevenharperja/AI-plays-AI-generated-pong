{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from\n",
    "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb#scrollTo=V8oadoJSWp7C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from autorom[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from autorom[accept-rom-license]) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from autorom[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from click->autorom[accept-rom-license]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (2.3.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch==2.3.0->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#!apt install python-opengl\n",
    "#!apt install ffmpeg\n",
    "#!apt install xvfb\n",
    "#!pip install pyvirtualdisplay\n",
    "#!pip install pyglet==1.5.1\n",
    "#!pip install imageio\n",
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install torchvision\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Virtual display\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "# virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Gym\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_id = \"ALE/Pong-v5\"\n",
    "# Create the env\n",
    "env = gym.make(env_id,obs_type=\"grayscale\",full_action_space=False)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id,obs_type=\"grayscale\",full_action_space=False)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = gym.spaces.utils.flatten_space(env.observation_space).shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  33600\n",
      "Sample observation [[ 68 187 222 ...  13   0 231]\n",
      " [112 113 166 ...  66 144 231]\n",
      " [  7  25  86 ... 222   1 229]\n",
      " ...\n",
      " [225 175  15 ... 142  46  17]\n",
      " [245 202 109 ... 143  19  38]\n",
      " [ 30 182  79 ... 152  45 229]]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  6\n",
      "Action Space Sample 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class Saver():\n",
    "    \"\"\"\n",
    "    Class to save the output and input to an environment as tensors which can be used to train the nn\n",
    "    \"\"\"\n",
    "    def __init__(self,save_dir,in_transform = None, out_transform = None, small_transform = None,file_limit = 1000) -> None:\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.observations = deque()#maxsize=3)\n",
    "        self.actions = deque()#maxsize=1)\n",
    "        self.rewards = deque()#maxsize=1)\n",
    "        self.dones = deque()#maxsize=1)\n",
    "        self.h=None\n",
    "        self.w=None\n",
    "        self.file_limit = file_limit\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "        self.small_transform = small_transform\n",
    "\n",
    "        os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "    def add_transition(self,observation, action, reward, done):\n",
    "        \"\"\"\n",
    "        Observations: What the agent sees (dimensions are 210 x 160)\n",
    "        Actions: What the agent does (dimensions are 6 x 1)\n",
    "        Rewards: The reward the agent gets (range is -1 to 1)?\n",
    "        Dones: If the episode is over (True or False)\n",
    "        \"\"\"\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        if len(self.observations) == 3:\n",
    "            self._save_transitions()\n",
    "            #clear screen of 3 screens ago.\n",
    "            self.observations.popleft()\n",
    "        \n",
    "        #clear old values\n",
    "        if len(self.actions) == 1:\n",
    "            self.actions.popleft()\n",
    "        if len(self.rewards) == 1:\n",
    "            self.rewards.popleft()\n",
    "        if len(self.dones) == 1:\n",
    "            self.dones.popleft()\n",
    "\n",
    "    def _transform(self, input,truth):\n",
    "        if self.in_transform == None or self.out_transform == None:\n",
    "            return(input,truth)\n",
    "        input = self.in_transform(input)\n",
    "        #make the pong image bigger\n",
    "        truth[0] = self.out_transform(truth[0])\n",
    "        #add a small version of the pong image into index 0.\n",
    "        truth = [self.small_transform(truth[0])] + truth\n",
    "\n",
    "        return input,truth\n",
    "\n",
    "    def _convert_to_tensors(self):\n",
    "        if self.h == None or self.w == None:\n",
    "            h = self.observations[0].shape[0] #height of a pong image\n",
    "            w = self.observations[0].shape[1] #width of a pong image\n",
    "\n",
    "        #take 2 images as input, return one image (the model should need two input images to determine ball velocity)\n",
    "        #expand action to a matrix the size of a pong image, and add it as a seperate channel in a tensor image later\n",
    "        act = np.broadcast_to(self.actions[0], (h,w) )\n",
    "        rew = self.rewards[0]\n",
    "        don = self.dones[0]\n",
    "\n",
    "\n",
    "        input = np.stack((self.observations[0], self.observations[1], act), axis=0) #put the channels together so my life is easier and the nn pays a lot of attention to act\n",
    "        truth = (self.observations[2],rew,don) #tuple of outputs\n",
    "        \n",
    "        \n",
    "\n",
    "        input = torch.tensor(input,dtype=torch.float)\n",
    "        truth = [\n",
    "            torch.tensor(np.expand_dims(truth[0],axis=0),dtype=torch.float),\n",
    "            torch.unsqueeze(torch.tensor(truth[1],dtype=torch.float),dim=0),\n",
    "            torch.unsqueeze(torch.tensor(truth[2],dtype=torch.float),dim=0),\n",
    "        ]\n",
    "        assert input.shape == (3,h,w)\n",
    "        assert truth[0].shape == (1,h,w)\n",
    "        assert truth[1].shape == (1,), \"shape is {}\".format(truth[1].shape)\n",
    "        assert truth[2].shape == (1,)\n",
    "\n",
    "        return input,truth\n",
    "\n",
    "    def _save_transitions(self):\n",
    "        #change bool in dones to floats\n",
    "        for i in range(len(self.dones)):\n",
    "            self.dones[i] = float(self.dones[i])\n",
    "\n",
    "        existing_files = [f for f in listdir(self.save_dir) if isfile(join(self.save_dir, f))]\n",
    "        j = None\n",
    "        if existing_files:\n",
    "            #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
    "            j = max([int(''.join([c for c in f if c.isdigit()])) for f in existing_files])\n",
    "        else:\n",
    "            j = -1\n",
    "        j += 1\n",
    "\n",
    "        if j >= self.file_limit:\n",
    "            sys.exit(0)\n",
    "\n",
    "        input,truth = self._convert_to_tensors()\n",
    "\n",
    "        input,truth = self._transform(input,truth)\n",
    "\n",
    "        torch.save(input,self.save_dir+\"input{i}.pt\".format(i=j))\n",
    "        torch.save((truth),self.save_dir+\"truth{i}.pt\".format(i=j))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "save_dir = \"diffusion_training_data/\"\n",
    "imagenet_stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "#TODO add a transform for, and also save, a small version of the truth image.\n",
    "\n",
    "in_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    # torchvision.transforms.Normalize(*imagenet_stats)\n",
    "                                      ])\n",
    "out_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "                                      ])\n",
    "small_transform = out_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64, 64)),\n",
    "                                      ])\n",
    "saver = Saver(save_dir,in_transform=in_transform,out_transform=out_transform, small_transform=small_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pong(n_episodes, max_t):\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0]\n",
    "        for t in range(max_t):\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            saver.add_transition(state, action, reward, terminated or truncated)\n",
    "            if terminated or truncated:\n",
    "                break \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwes\\Projects\\AI-plays-AI-generated-pong\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Tensor' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m n_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m max_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrun_pong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36mrun_pong\u001b[1;34m(n_episodes, max_t)\u001b[0m\n\u001b[0;32m      5\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m      6\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m----> 7\u001b[0m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m, in \u001b[0;36mSaver.add_transition\u001b[1;34m(self, observation, action, reward, done)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones\u001b[38;5;241m.\u001b[39mappend(done)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#clear screen of 3 screens ago.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "Cell \u001b[1;32mIn[10], line 110\u001b[0m, in \u001b[0;36mSaver._save_transitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28minput\u001b[39m,truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_tensors()\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28minput\u001b[39m,truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m=\u001b[39mj))\n\u001b[0;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39msave((truth),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruth\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m=\u001b[39mj))\n",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m, in \u001b[0;36mSaver._transform\u001b[1;34m(self, input, truth)\u001b[0m\n\u001b[0;32m     55\u001b[0m truth[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_transform(truth[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#add a small version of the pong image into index 0.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmall_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m,truth\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Tensor' and 'list'"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "max_t = 1000\n",
    "run_pong(n_episodes,max_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
