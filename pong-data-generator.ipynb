{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from\n",
    "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb#scrollTo=V8oadoJSWp7C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: pyglet==1.5.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: imageio in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.34.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from imageio) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from imageio) (10.3.0)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from autorom[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from autorom[accept-rom-license]) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from autorom[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from click->autorom[accept-rom-license]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\projects\\ai-plays-ai-generated-pong\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1\n",
    "!pip install imageio\n",
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Virtual display\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "# virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Gym\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"ALE/Pong-v5\"\n",
    "# Create the env\n",
    "env = gym.make(env_id,obs_type=\"grayscale\",full_action_space=False)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id,obs_type=\"grayscale\",full_action_space=False)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = gym.spaces.utils.flatten_space(env.observation_space).shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  33600\n",
      "Sample observation [[ 77 159 148 ...  67 253 140]\n",
      " [  6 234  62 ... 173 237 232]\n",
      " [141  72  98 ...  86   7  94]\n",
      " ...\n",
      " [120 228 165 ... 229 187 253]\n",
      " [141  14  31 ... 101  86 199]\n",
      " [ 48 209 154 ...  51  30  23]]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  6\n",
      "Action Space Sample 5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class Saver():\n",
    "    \"\"\"\n",
    "    Class to save the output and input to an environment as tensors which can be used to train the nn\n",
    "    \"\"\"\n",
    "    def __init__(self,save_dir,file_limit = 1000) -> None:\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.observations = deque()#maxsize=3)\n",
    "        self.actions = deque()#maxsize=1)\n",
    "        self.rewards = deque()#maxsize=1)\n",
    "        self.dones = deque()#maxsize=1)\n",
    "        self.h=None\n",
    "        self.w=None\n",
    "        self.file_limit = file_limit\n",
    "\n",
    "        os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "    def add_transition(self,observation, action, reward, done):\n",
    "        \"\"\"\n",
    "        Observations: What the agent sees (dimensions are 210 x 160)\n",
    "        Actions: What the agent does (dimensions are 6 x 1)\n",
    "        Rewards: The reward the agent gets (range is -1 to 1)?\n",
    "        Dones: If the episode is over (True or False)\n",
    "        \"\"\"\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        if len(self.observations) == 3:\n",
    "            self._save_transitions()\n",
    "            #clear screen of 3 screens ago.\n",
    "            self.observations.popleft()\n",
    "        \n",
    "        #clear old values\n",
    "        if len(self.actions) == 1:\n",
    "            self.actions.popleft()\n",
    "        if len(self.rewards) == 1:\n",
    "            self.rewards.popleft()\n",
    "        if len(self.dones) == 1:\n",
    "            self.dones.popleft()\n",
    "\n",
    "\n",
    "    def _convert_to_tensors(self):\n",
    "        if self.h == None or self.w == None:\n",
    "            h = self.observations[0].shape[0] #height of a pong image\n",
    "            w = self.observations[0].shape[1] #width of a pong image\n",
    "\n",
    "        #take 2 images as input, return one image (the model should need two input images to determine ball velocity)\n",
    "        #expand action to a matrix the size of a pong image, and add it as a seperate channel in a tensor image later\n",
    "        act = np.broadcast_to(self.actions[0], (h,w) )\n",
    "        rew = self.rewards[0]\n",
    "        don = self.dones[0]\n",
    "\n",
    "\n",
    "        input = np.stack((self.observations[0], self.observations[1], act), axis=0) #put the channels together so my life is easier and the nn pays a lot of attention to act\n",
    "        truth = (self.observations[2],rew,don) #tuple of outputs\n",
    "        \n",
    "        \n",
    "\n",
    "        input = torch.tensor(input,dtype=torch.float)\n",
    "        truth = (\n",
    "            torch.tensor(np.expand_dims(truth[0],axis=0),dtype=torch.float),\n",
    "            torch.unsqueeze(torch.tensor(truth[1],dtype=torch.float),dim=0),\n",
    "            torch.unsqueeze(torch.tensor(truth[2],dtype=torch.float),dim=0),\n",
    "        )\n",
    "        assert input.shape == (3,h,w)\n",
    "        assert truth[0].shape == (1,h,w)\n",
    "        assert truth[1].shape == (1,), \"shape is {}\".format(truth[1].shape)\n",
    "        assert truth[2].shape == (1,)\n",
    "\n",
    "        return input,truth\n",
    "\n",
    "    def _save_transitions(self):\n",
    "        #change bool in dones to floats\n",
    "        for i in range(len(self.dones)):\n",
    "            self.dones[i] = float(self.dones[i])\n",
    "\n",
    "        existing_files = [f for f in listdir(self.save_dir) if isfile(join(self.save_dir, f))]\n",
    "        j = None\n",
    "        if existing_files:\n",
    "            #This grabs the largest integer out of all the filenames (filter the string for digit chars, convert those chars to an int)\n",
    "            j = max([int(''.join([c for c in f if c.isdigit()])) for f in existing_files])\n",
    "        else:\n",
    "            j = -1\n",
    "        j += 1\n",
    "\n",
    "        if j >= self.file_limit:\n",
    "            sys.exit(0)\n",
    "\n",
    "        input,truth = self._convert_to_tensors()\n",
    "        torch.save(input,self.save_dir+\"input{i}.pt\".format(i=j))\n",
    "        torch.save((truth),self.save_dir+\"truth{i}.pt\".format(i=j))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"diffusion_training_data/\"\n",
    "saver = Saver(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pong(n_episodes, max_t):\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0]\n",
    "        for t in range(max_t):\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            saver.add_transition(state, action, reward, terminated or truncated)\n",
    "            if terminated or truncated:\n",
    "                break \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Saver' object has no attribute 'file_limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m n_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m max_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrun_pong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 7\u001b[0m, in \u001b[0;36mrun_pong\u001b[1;34m(n_episodes, max_t)\u001b[0m\n\u001b[0;32m      5\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m      6\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m----> 7\u001b[0m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 34\u001b[0m, in \u001b[0;36mSaver.add_transition\u001b[1;34m(self, observation, action, reward, done)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones\u001b[38;5;241m.\u001b[39mappend(done)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m#clear screen of 3 screens ago.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "Cell \u001b[1;32mIn[37], line 91\u001b[0m, in \u001b[0;36mSaver._save_transitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     89\u001b[0m j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_limit\u001b[49m:\n\u001b[0;32m     92\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28minput\u001b[39m,truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_tensors()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Saver' object has no attribute 'file_limit'"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "max_t = 1000\n",
    "run_pong(n_episodes,max_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
