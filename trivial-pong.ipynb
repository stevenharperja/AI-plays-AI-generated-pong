{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shwes\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"diffusion training data/\"\n",
    "def load_transitions():\n",
    "    import os.path\n",
    "    if not os.path.isfile(save_dir + \"observations.npz\"):\n",
    "        print(\"No saved transitions found\")\n",
    "        return()\n",
    "    #its stored as a dict so grab out the single array. and 0th axis should be the number of transitions\n",
    "    observations = np.load(save_dir + \"observations.npz\")[\"arr_0\"]\n",
    "    actions = np.load(save_dir + \"actions.npz\")[\"arr_0\"]\n",
    "    rewards = np.load(save_dir + \"rewards.npz\")[\"arr_0\"]\n",
    "    dones = np.load(save_dir + \"dones.npz\")[\"arr_0\"]\n",
    "    assert(len(observations) == len(actions) == len(rewards) == len(dones))\n",
    "    print(\"loaded transitions of length\", len(observations))\n",
    "\n",
    "\n",
    "    #crashes on codespaces\n",
    "    # observations = torch.tensor(observations, dtype=torch.float32, device=device)\n",
    "    # actions = torch.tensor(actions, dtype=torch.float32, device=device)\n",
    "    # rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    # dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "    return observations, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded transitions of length 15280\n",
      "33600 1 1 1\n"
     ]
    }
   ],
   "source": [
    "observations, actions, rewards, dones = load_transitions()\n",
    "print(observations[0].size, actions[0].size, rewards[0].size, dones[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#change bool in dones to floats\n",
    "for i in range(len(dones)):\n",
    "    dones[i] = dones[i].astype(float)\n",
    "#change numpy nd arrays to lists\n",
    "l_obs = []\n",
    "l_actions = []\n",
    "l_rewards = []\n",
    "l_dones = []\n",
    "for i in range(len(actions)):\n",
    "    l_obs.append(observations[i])\n",
    "    l_actions.append(actions[i])\n",
    "    l_rewards.append(rewards[i])\n",
    "    l_dones.append(dones[i])\n",
    "#flatten the observations\n",
    "for i in range(len(l_obs)):\n",
    "    l_obs[i] = l_obs[i].flatten()\n",
    "\n",
    "\n",
    "#create list to organize the training data. \n",
    "#data is a list of [inputs, labels]\n",
    "data = [[],[]]\n",
    "for i in range(len(l_obs)-2):\n",
    "    input = np.concatenate((l_obs[i], l_obs[i+1], [l_actions[i+1]]))\n",
    "    label = np.concatenate((l_obs[i+2], [l_rewards[i+2]], [l_dones[i+2]]))\n",
    "    data[0].append(input)\n",
    "    data[1].append(label)\n",
    "\n",
    "#data too big, try smaller amount\n",
    "data = [data[0][:100], data[1][:100]]\n",
    "#convert to tensors\n",
    "# for i in range(2):\n",
    "#     data[i] = torch.tensor(data[i], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Conv2d(in_channels = input_size, out_channels =hidden_size, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.fc2 = nn.ConvTranspose2d(in_channels=hidden_size, out_channels=output_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the input, hidden, and output sizes\n",
    "input_size = 2 * observations[0].size + actions[0].size #two screens of pong and the action taken on the last frame\n",
    "hidden_size = 5 #arbitrary\n",
    "output_size = observations[0].size + rewards[0].size + dones[0].size #one screen of pong + the reward + done\n",
    "\n",
    "# Create an instance of the network\n",
    "net = Net().to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "# define loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "#see https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        # self.img_labels = pd.read_csv(annotations_file)\n",
    "        # self.img_dir = img_dir\n",
    "        self.img_labels = data[1]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        # img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = data[0]\n",
    "        label = data[1]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "batch_size = 64\n",
    "trainset = CustomImageDataset(\"\",\"\")\n",
    "if __name__ == '__main__':\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        print(i)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pong_gen.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAAAAADoTpQ7AAACEUlEQVR4Ae3dUUrEQBREUSMuS9fhgnU5LkIaFC7DNDyqFJrh5seiSY3h9BsSP4LX08/x/hsO+3lNLuwNF/2BnMZXFD+R78Xne4snrXmB7W68TD9gzR5ncdrbnbdmj7O4O88t3slM1xWcSu3OU3AnM11XcCq1O0/Bncx0/XjB8b34L+/DS29yH17nHS/oBa5tao7R3yTNL2i7bnEreH21n/DPfbe4BVZQwVag7TuDDy/ow8LDb7HfYre4FWj7zqCCrUDbdwYVbAXavjOoYCvQ9p1BBVuBtu8MKtgKtH1nUMFWoO07gwq2Am3fGVSwFWj7zqCCrUDbdwYVbAXavjOoYCvQ9p1BBVuBtu8MKtgKtH1nUMFWoO07gwq2Am3fGXx4wfFra63EpL9eZVuvlvNwBqmRZAUTNXYUpEaSFUzU2FGQGklWMFFjR0FqJPl4waOeB2+fBZf48YJeYPLFYEdBaiRZwUSNHQWpkWQFEzV2FKRGkhVM1NhRkBpJVjBRY0dBaiRZwUSNHQWpkWQFEzV2FKRGkhVM1NhRkBpJVjBRY0dBaiRZwUSNHQWpkWQFEzV2FKRGkhVM1NhRkBpJVjBRY0dBaiRZwUSNHQWpkWQFEzV2FKRGkhVM1NhRkBpJVjBRY0dBaiRZwUSNHQWpkWT/01Wixo4zSI0kK5iosaMgNZKsYKLGjoLUSLKCiRo7ClIjyd9tWgsYNKf3UQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=160x210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.fromarray(observations[0])\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif = []\n",
    "images = [Image.fromarray(observation) for observation in observations]\n",
    "for image in images:\n",
    "    gif.append(image)\n",
    "gif[0].save('temp/result.gif', save_all=True,optimize=False, append_images=gif[1:], loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
